-
  name: >
    Designing AR intervention to support people with OCD
  location: UW-Madison
  advisor: Prof. Yuhang Zhao
  description: > 
    We seek to understand the core strategies of effective OCD therapies (e.g., ERP and ACT) and design AR real-time intervention to support OCD people's mental health when they are outside of therapy sessions.
  image: /img/ocd-teaser.png

-
  name: >
    Gaze‐Aware Visual Augmentations to Enhance Low Vision People's Reading Experience
  location: UW-Madison
  advisor: Prof. Yuhang Zhao
  description: > 
    We improve the gaze data collection process to make eye tracking accessible to low vision users, and identify low vision people's unique gaze patterns when reading using eye-tracking. 
    Based upon our findings, we further design gaze-aware visual augmentations that enhance low vision users' reading experience. Our system facilitates line switching/following and difficult word recognition.
  image: /img/GazePrompt-16-9.gif #/img/low_vision_gaze.jpg
  video: https://youtu.be/EmA39e5aKig
-
  name: >
    AR Systems to Facilitate Safe Cooking for People with Visual Impairment
  location: UW-Madison
  advisor: Prof. Yuhang Zhao
  description: > 
    We conduct contexual inquery study to observe how people with visual impairments (PVI) cook in their own kitchens. We also interview rehabilitation professionals about kitchen related training. 
    Combining the two studies, we seek to understand PVI's unique challenges and needs in the kitchen and identify design considerations for future assistive technology. 
  image: /img/low_vision_cook.jpg
-
  name: >
    CogTeach: Real-Time Cognitive Feedback for Online Instruction
  location: UCSD
  advisor: Prof. Xinyu Zhang
  description: > 
    We propose an online instruction platform CogTeach that collects students' eye gaze and facial expression to infer their cognitive state (confusion and engagement). 
    The system will then visualize the students' cognitive states and fixations on the instructor's side in real-time to help improve instruction and decision making.
  image: /img/fol.png
# - 
#   name: GazeScape
#   location: UCSD
#   advisor: Prof. Nadir Weibel
#   description: >
#     Eye gaze plays an important role in successful and effective turn-takings in group discussions. However, as all classes are moved online during the pandemic, students use video conferencing softwares, such as Zoom, for remote discussion, 
#     which lacks this important visual cue for communication. We propose GazeScape, a video communication system that enables gaze sharing within the group to facilitate turn-taking efficiency and communication quality. 
#   image: /img/gazescape.jpg
# -
#   name: Adaptive system for cognitive workload management using a portable EEG headset (2020)
#   location: UCSD
#   advisor: Prof. Xinyu Zhang
#   description: >
#     We built a closed-loop system that aims to mitigate the user's cognitive workload level when performing daily tasks. 
#     We measure the user's workload with EEG signal collected from Muse (a commercial EEG headset). Our workload classification model is trained with N-back tasks, and classification accuracy is 0.98. 
#     With our system and a simple mitigation strategy, the user's performance on a series of N-back tasks can be improved by 15% (in terms of reaction time).
#   image: /img/eeg.jpg
-
  name: ARTEMIS (Augmented Reality Technology-Enabled reMote Integrated Surgery)
  location: UCSD
  advisor: Prof. Nadir Weibel
  description: >
    ARTEMIS is a collaborative system for surgical telementoring. The surgical ﬁeld is recreated for a remote expert in VR, and the remote expert annotations and avatar are displayed into the novice’s ﬁeld of view in real-time using AR. 
    ARTEMIS supports remote surgical mentoring of novices through synchronous point, draw, and look affordances and asynchronous video clips.
  image: /img/artemis.png
# -
#   name: GrandpARents (2019)
#   location: UCSD
#   advisor: Prof. Nadir Weibel (course project for CSE218 Fall 2019)
#   description: >
#     GrandpARents is a system deployed on HoloLens that helps older adults with visual impairment and/or hearing loss to cope with their daily tasks.
#     There are two main features in our system. The first one is real-time subtitle for one-on-one conversations. The subtitle will be automatically generated in front of the user when someone is speaking;
#     the second feature is Zoom, which allows the user to magnify an area of interest by simply taking a picture and resize it with hand gestures.
#   image: /img/gp.jpg
#   # url: https://github.com/WeibelLab-Teaching/CSE218_Fa19_ARengers
# -
#   name: ThumbTrak (2018 - 2019)
#   location: Cornell University
#   advisor: Prof. Cheng Zhang
#   description: >
#     ThumbTrak is a continuous one-handed thumb-on-fingers input technology.
#     It allows the user to use the thumb to continuously input on fingers by tracking the position of the thumb using wearable motion sensing. It consists of a thumb-ring and a wristband.
#     With ThumbTrak, the user is able to input on applications in one-hand, which demands continuously input capability, such as drawing and text entry applications.
#   image: /img/proj1.jpg
#   url: /pdfs/Intro_ThumbTrak.pdf
# -
#   name: On-Shelf Product Image Generation for Product Classification using GAN (2019)
#   location: Shanghai Jiao Tong University & Clobotics Co.,Ltd
#   advisor: Dr. Cong Yang, Prof. Weiyao Lin
#   description: >
#     We collected 50k+ real product (beverage) images with a self-built turntable, and cameras from 5 different angles.
#     We used the images collected with our device and real on-shelf product images from our database to train a Cycle-GAN model.
#     The model can generate 'fake' on-shelf product images which can be further used to train our product classification models, aiming at reducing the cost of manual data annotation.
#   image: /img/gan_stage_res.png
# -
#   name: Approximate Random Dropout for DNN training acceleration in GPGPU (2017 - 2018)
#   location: Shanghai Jiao Tong University
#   advisor: Prof. Li Jiang
#   description: >
#     The training phases of DNN consumes enormous processing time and energy, and conventional compression techniques can hardly be used in the training phase.
#     We propose Approximate Random Dropout to eliminate the unnecessary computation and data access. To compensate the performance loss we develop a SGD-based Search Algorithm to produce the distribution of dropout patterns.
#   image: /img/proj2.png
#   # url: https://ieeexplore.ieee.org/abstract/document/8715135
# -
#   name: Virtual Dressing System
#   location: Shanghai Jiao Tong University
#   advisor: Prof. Weiyao Lin
#   description: >
#     This system is based on a RGBD camera (Kinect V2) and OpenPose.
#     We use Kinect SDK to retrieve 3D human pose and refine the 3D skeleton with 2D pose estimation obtained by OpenPose, and reproject the refined 2D skeleton back to 3D space using depth information.
#     This system can let user select clothes and try them on. The size of the clothes can be changed automatically according to the user's body shape.
#   image: /img/proj3.png
  # url: http://google.com
# -
#   name: 'RGBD-based Single Person 3D Pose Estimation (On Going)'
#   location: Shanghai Jiao Tong University
#   advisor: Weiyao Lin
#   description: >
#     This work is to estimate 3D human pose directly from RGB and depth images obtained by Kinect V2.
#     We first utilize a state-of-the-art 2D pose estimator to get 2D keypoints heatmaps and tile them along z-dimention. At the same time we voxelize the depth image.
#     Then the two features are combined together and fed into a Encoder-Decoder network to produce the 3D keypoints.
#   image: /img/proj4.png
  # url: http://google.com
# -
#   name: RNN-based Driver's Cognitive Workload Analysis
#   location: Shanghai Jiao Tong University
#   advisor: Na Ruan
#   description: >
#     This work is to estimate driver's cognitive workload while driving.
#     We use physiological data and driver's self-ratings while driving to generate ground truth and build a RNN-based realtime estimation system using only vehicle data such as acceleration and speed.
#
#   image: /img/proj5.png
#   # url: http://google.com
